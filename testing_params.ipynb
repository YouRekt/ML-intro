{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for generating stuff to be used in the report\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(root='spectrograms/train', transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_mean = torch.stack([img.mean(1).mean(1) for img, _ in train_dataset]).mean(0)\n",
    "train_std = torch.stack([img.std(1).std(1) for img, _ in train_dataset]).mean(0)\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=train_mean, std=train_std)\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.ImageFolder(\"spectrograms/train\", transform=data_transforms)\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(train_dataset, [0.8,0.2])\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(\"spectrograms/test\", transform=transforms.ToTensor())\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=train_mean, std=train_std)\n",
    "])\n",
    "\n",
    "test_dataset = torchvision.datasets.ImageFolder(\"spectrograms/test\", transform=test_transforms)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=26912, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
    "        self.fc1 = nn.Linear(32 * 29 * 29, 128)\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "        self.fc3 = nn.Linear(16, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "epochs = 8\n",
    "params = [\n",
    "    {\"lr\": 0.001, \"momentum\": 0.9, \"title\": \"lr=0.001, momentum=0.9\"},\n",
    "    {\"lr\": 0.001, \"momentum\": 0.5, \"title\": \"lr=0.001, momentum=0.5\"},\n",
    "    {\"lr\": 0.001, \"momentum\": 0.1, \"title\": \"lr=0.001, momentum=0.1\"},\n",
    "    {\"lr\": 0.003, \"momentum\": 0.9, \"title\": \"lr=0.003, momentum=0.9\"},\n",
    "    {\"lr\": 0.003, \"momentum\": 0.5, \"title\": \"lr=0.003, momentum=0.5\"},\n",
    "    {\"lr\": 0.003, \"momentum\": 0.1, \"title\": \"lr=0.003, momentum=0.1\"},\n",
    "    {\"lr\": 0.01, \"momentum\": 0.9, \"title\": \"lr=0.01, momentum=0.9\"},\n",
    "    {\"lr\": 0.01, \"momentum\": 0.5, \"title\": \"lr=0.01, momentum=0.5\"},\n",
    "    {\"lr\": 0.01, \"momentum\": 0.1, \"title\": \"lr=0.01, momentum=0.1\"},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwaluzenicz-ignacy\u001b[0m (\u001b[33mIntro-ML\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ignacy-waluzenicz/code/iml/ML-intro/wandb/run-20250127_214228-63tqzqzp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Intro-ML/Voice-Recognition/runs/63tqzqzp' target=\"_blank\">proud-star-21</a></strong> to <a href='https://wandb.ai/Intro-ML/Voice-Recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Intro-ML/Voice-Recognition' target=\"_blank\">https://wandb.ai/Intro-ML/Voice-Recognition</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Intro-ML/Voice-Recognition/runs/63tqzqzp' target=\"_blank\">https://wandb.ai/Intro-ML/Voice-Recognition/runs/63tqzqzp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.696\n",
      "[1,   400] loss: 0.691\n",
      "[1,   600] loss: 0.692\n",
      "[1,   800] loss: 0.695\n",
      "[1,  1000] loss: 0.693\n",
      "[1,  1200] loss: 0.691\n",
      "[1,  1400] loss: 0.690\n",
      "[1,  1600] loss: 0.693\n",
      "[1,  1800] loss: 0.687\n",
      "[1,  2000] loss: 0.691\n",
      "[1,  2200] loss: 0.693\n",
      "[1,  2400] loss: 0.695\n",
      "[1,  2600] loss: 0.694\n",
      "[1,  2800] loss: 0.694\n",
      "[1,  3000] loss: 0.692\n",
      "[1,  3200] loss: 0.693\n",
      "[1,  3400] loss: 0.689\n",
      "[1,  3600] loss: 0.691\n",
      "[1,  3800] loss: 0.692\n",
      "[1,  4000] loss: 0.692\n",
      "[1,  4200] loss: 0.690\n",
      "[1,  4400] loss: 0.689\n",
      "[1,  4600] loss: 0.694\n",
      "[1,  4800] loss: 0.692\n",
      "[1,  5000] loss: 0.693\n",
      "[1,  5200] loss: 0.691\n",
      "[1,  5400] loss: 0.694\n",
      "[1,  5600] loss: 0.693\n",
      "[1,  5800] loss: 0.693\n",
      "[1,  6000] loss: 0.688\n",
      "[1,  6200] loss: 0.689\n",
      "[1,  6400] loss: 0.688\n",
      "[1,  6600] loss: 0.689\n",
      "[1,  6800] loss: 0.665\n",
      "[1,  7000] loss: 0.659\n",
      "[1,  7200] loss: 0.604\n",
      "[1,  7400] loss: 0.550\n",
      "[1,  7600] loss: 0.563\n",
      "[1,  7800] loss: 0.449\n",
      "[1,  8000] loss: 0.426\n",
      "[1,  8200] loss: 0.367\n",
      "[2,   200] loss: 0.247\n",
      "[2,   400] loss: 0.341\n",
      "[2,   600] loss: 0.303\n",
      "[2,   800] loss: 0.275\n",
      "[2,  1000] loss: 0.228\n",
      "[2,  1200] loss: 0.215\n",
      "[2,  1400] loss: 0.179\n",
      "[2,  1600] loss: 0.188\n",
      "[2,  1800] loss: 0.173\n",
      "[2,  2000] loss: 0.213\n",
      "[2,  2200] loss: 0.175\n",
      "[2,  2400] loss: 0.123\n",
      "[2,  2600] loss: 0.169\n",
      "[2,  2800] loss: 0.212\n",
      "[2,  3000] loss: 0.162\n",
      "[2,  3200] loss: 0.151\n",
      "[2,  3400] loss: 0.153\n",
      "[2,  3600] loss: 0.164\n",
      "[2,  3800] loss: 0.126\n",
      "[2,  4000] loss: 0.134\n",
      "[2,  4200] loss: 0.120\n",
      "[2,  4400] loss: 0.145\n",
      "[2,  4600] loss: 0.123\n",
      "[2,  4800] loss: 0.111\n",
      "[2,  5000] loss: 0.083\n",
      "[2,  5200] loss: 0.103\n",
      "[2,  5400] loss: 0.081\n",
      "[2,  5600] loss: 0.089\n",
      "[2,  5800] loss: 0.110\n",
      "[2,  6000] loss: 0.121\n",
      "[2,  6200] loss: 0.090\n",
      "[2,  6400] loss: 0.095\n",
      "[2,  6600] loss: 0.109\n",
      "[2,  6800] loss: 0.080\n",
      "[2,  7000] loss: 0.069\n",
      "[2,  7200] loss: 0.091\n",
      "[2,  7400] loss: 0.094\n",
      "[2,  7600] loss: 0.076\n",
      "[2,  7800] loss: 0.090\n",
      "[2,  8000] loss: 0.077\n",
      "[2,  8200] loss: 0.110\n",
      "[3,   200] loss: 0.079\n",
      "[3,   400] loss: 0.090\n",
      "[3,   600] loss: 0.081\n",
      "[3,   800] loss: 0.061\n",
      "[3,  1000] loss: 0.048\n",
      "[3,  1200] loss: 0.084\n",
      "[3,  1400] loss: 0.074\n",
      "[3,  1600] loss: 0.080\n",
      "[3,  1800] loss: 0.081\n",
      "[3,  2000] loss: 0.055\n",
      "[3,  2200] loss: 0.057\n",
      "[3,  2400] loss: 0.056\n",
      "[3,  2600] loss: 0.040\n",
      "[3,  2800] loss: 0.049\n",
      "[3,  3000] loss: 0.061\n",
      "[3,  3200] loss: 0.054\n",
      "[3,  3400] loss: 0.048\n",
      "[3,  3600] loss: 0.072\n",
      "[3,  3800] loss: 0.047\n",
      "[3,  4000] loss: 0.044\n",
      "[3,  4200] loss: 0.046\n",
      "[3,  4400] loss: 0.038\n",
      "[3,  4600] loss: 0.037\n",
      "[3,  4800] loss: 0.051\n",
      "[3,  5000] loss: 0.070\n",
      "[3,  5200] loss: 0.059\n",
      "[3,  5400] loss: 0.042\n",
      "[3,  5600] loss: 0.058\n",
      "[3,  5800] loss: 0.039\n",
      "[3,  6000] loss: 0.082\n",
      "[3,  6200] loss: 0.063\n",
      "[3,  6400] loss: 0.055\n",
      "[3,  6600] loss: 0.030\n",
      "[3,  6800] loss: 0.083\n",
      "[3,  7000] loss: 0.047\n",
      "[3,  7200] loss: 0.058\n",
      "[3,  7400] loss: 0.048\n",
      "[3,  7600] loss: 0.038\n",
      "[3,  7800] loss: 0.027\n",
      "[3,  8000] loss: 0.055\n",
      "[3,  8200] loss: 0.045\n",
      "[4,   200] loss: 0.030\n",
      "[4,   400] loss: 0.037\n",
      "[4,   600] loss: 0.014\n",
      "[4,   800] loss: 0.052\n",
      "[4,  1000] loss: 0.039\n",
      "[4,  1200] loss: 0.028\n",
      "[4,  1400] loss: 0.024\n",
      "[4,  1600] loss: 0.034\n",
      "[4,  1800] loss: 0.027\n",
      "[4,  2000] loss: 0.047\n",
      "[4,  2200] loss: 0.039\n",
      "[4,  2400] loss: 0.019\n",
      "[4,  2600] loss: 0.017\n",
      "[4,  2800] loss: 0.043\n",
      "[4,  3000] loss: 0.040\n",
      "[4,  3200] loss: 0.030\n",
      "[4,  3400] loss: 0.024\n",
      "[4,  3600] loss: 0.021\n",
      "[4,  3800] loss: 0.030\n",
      "[4,  4000] loss: 0.024\n",
      "[4,  4200] loss: 0.036\n",
      "[4,  4400] loss: 0.037\n",
      "[4,  4600] loss: 0.036\n",
      "[4,  4800] loss: 0.023\n",
      "[4,  5000] loss: 0.033\n",
      "[4,  5200] loss: 0.031\n",
      "[4,  5400] loss: 0.064\n",
      "[4,  5600] loss: 0.031\n",
      "[4,  5800] loss: 0.029\n",
      "[4,  6000] loss: 0.037\n",
      "[4,  6200] loss: 0.042\n",
      "[4,  6400] loss: 0.028\n",
      "[4,  6600] loss: 0.042\n",
      "[4,  6800] loss: 0.011\n",
      "[4,  7000] loss: 0.029\n",
      "[4,  7200] loss: 0.048\n",
      "[4,  7400] loss: 0.028\n",
      "[4,  7600] loss: 0.021\n",
      "[4,  7800] loss: 0.016\n",
      "[4,  8000] loss: 0.027\n",
      "[4,  8200] loss: 0.047\n",
      "[5,   200] loss: 0.014\n",
      "[5,   400] loss: 0.021\n",
      "[5,   600] loss: 0.032\n",
      "[5,   800] loss: 0.039\n",
      "[5,  1000] loss: 0.025\n",
      "[5,  1200] loss: 0.029\n",
      "[5,  1400] loss: 0.034\n",
      "[5,  1600] loss: 0.015\n",
      "[5,  1800] loss: 0.013\n",
      "[5,  2000] loss: 0.018\n",
      "[5,  2200] loss: 0.019\n",
      "[5,  2400] loss: 0.026\n",
      "[5,  2600] loss: 0.019\n",
      "[5,  2800] loss: 0.013\n",
      "[5,  3000] loss: 0.032\n",
      "[5,  3200] loss: 0.014\n",
      "[5,  3400] loss: 0.013\n",
      "[5,  3600] loss: 0.017\n",
      "[5,  3800] loss: 0.039\n",
      "[5,  4000] loss: 0.066\n",
      "[5,  4200] loss: 0.044\n",
      "[5,  4400] loss: 0.035\n",
      "[5,  4600] loss: 0.033\n",
      "[5,  4800] loss: 0.016\n",
      "[5,  5000] loss: 0.014\n",
      "[5,  5200] loss: 0.027\n",
      "[5,  5400] loss: 0.022\n",
      "[5,  5600] loss: 0.017\n",
      "[5,  5800] loss: 0.044\n",
      "[5,  6000] loss: 0.027\n",
      "[5,  6200] loss: 0.019\n",
      "[5,  6400] loss: 0.019\n",
      "[5,  6600] loss: 0.023\n",
      "[5,  6800] loss: 0.015\n",
      "[5,  7000] loss: 0.027\n",
      "[5,  7200] loss: 0.036\n",
      "[5,  7400] loss: 0.027\n",
      "[5,  7600] loss: 0.063\n",
      "[5,  7800] loss: 0.023\n",
      "[5,  8000] loss: 0.016\n",
      "[5,  8200] loss: 0.030\n",
      "[6,   200] loss: 0.019\n",
      "[6,   400] loss: 0.008\n",
      "[6,   600] loss: 0.009\n",
      "[6,   800] loss: 0.026\n",
      "[6,  1000] loss: 0.007\n",
      "[6,  1200] loss: 0.030\n",
      "[6,  1400] loss: 0.018\n",
      "[6,  1600] loss: 0.017\n",
      "[6,  1800] loss: 0.024\n",
      "[6,  2000] loss: 0.032\n",
      "[6,  2200] loss: 0.008\n",
      "[6,  2400] loss: 0.023\n",
      "[6,  2600] loss: 0.025\n",
      "[6,  2800] loss: 0.026\n",
      "[6,  3000] loss: 0.012\n",
      "[6,  3200] loss: 0.008\n",
      "[6,  3400] loss: 0.025\n",
      "[6,  3600] loss: 0.013\n",
      "[6,  3800] loss: 0.004\n",
      "[6,  4000] loss: 0.006\n",
      "[6,  4200] loss: 0.014\n",
      "[6,  4400] loss: 0.026\n",
      "[6,  4600] loss: 0.029\n",
      "[6,  4800] loss: 0.009\n",
      "[6,  5000] loss: 0.007\n",
      "[6,  5200] loss: 0.013\n",
      "[6,  5400] loss: 0.007\n",
      "[6,  5600] loss: 0.008\n",
      "[6,  5800] loss: 0.013\n",
      "[6,  6000] loss: 0.043\n",
      "[6,  6200] loss: 0.019\n",
      "[6,  6400] loss: 0.008\n",
      "[6,  6600] loss: 0.017\n",
      "[6,  6800] loss: 0.014\n",
      "[6,  7000] loss: 0.012\n",
      "[6,  7200] loss: 0.027\n",
      "[6,  7400] loss: 0.024\n",
      "[6,  7600] loss: 0.019\n",
      "[6,  7800] loss: 0.023\n",
      "[6,  8000] loss: 0.031\n",
      "[6,  8200] loss: 0.009\n",
      "[7,   200] loss: 0.005\n",
      "[7,   400] loss: 0.006\n",
      "[7,   600] loss: 0.082\n",
      "[7,   800] loss: 0.032\n",
      "[7,  1000] loss: 0.038\n",
      "[7,  1200] loss: 0.020\n",
      "[7,  1400] loss: 0.011\n",
      "[7,  1600] loss: 0.012\n",
      "[7,  1800] loss: 0.012\n",
      "[7,  2000] loss: 0.032\n",
      "[7,  2200] loss: 0.015\n",
      "[7,  2400] loss: 0.004\n",
      "[7,  2600] loss: 0.008\n",
      "[7,  2800] loss: 0.006\n",
      "[7,  3000] loss: 0.026\n",
      "[7,  3200] loss: 0.018\n",
      "[7,  3400] loss: 0.014\n",
      "[7,  3600] loss: 0.026\n",
      "[7,  3800] loss: 0.015\n",
      "[7,  4000] loss: 0.019\n",
      "[7,  4200] loss: 0.013\n",
      "[7,  4400] loss: 0.008\n",
      "[7,  4600] loss: 0.040\n",
      "[7,  4800] loss: 0.012\n",
      "[7,  5000] loss: 0.011\n",
      "[7,  5200] loss: 0.033\n",
      "[7,  5400] loss: 0.011\n",
      "[7,  5600] loss: 0.020\n",
      "[7,  5800] loss: 0.050\n",
      "[7,  6000] loss: 0.013\n",
      "[7,  6200] loss: 0.015\n",
      "[7,  6400] loss: 0.018\n",
      "[7,  6600] loss: 0.009\n",
      "[7,  6800] loss: 0.014\n",
      "[7,  7000] loss: 0.011\n",
      "[7,  7200] loss: 0.007\n",
      "[7,  7400] loss: 0.027\n",
      "[7,  7600] loss: 0.016\n",
      "[7,  7800] loss: 0.025\n",
      "[7,  8000] loss: 0.021\n",
      "[7,  8200] loss: 0.007\n",
      "[8,   200] loss: 0.004\n",
      "[8,   400] loss: 0.022\n",
      "[8,   600] loss: 0.005\n",
      "[8,   800] loss: 0.018\n",
      "[8,  1000] loss: 0.008\n",
      "[8,  1200] loss: 0.004\n",
      "[8,  1400] loss: 0.005\n",
      "[8,  1600] loss: 0.005\n",
      "[8,  1800] loss: 0.004\n",
      "[8,  2000] loss: 0.004\n",
      "[8,  2200] loss: 0.003\n",
      "[8,  2400] loss: 0.005\n",
      "[8,  2600] loss: 0.007\n",
      "[8,  2800] loss: 0.005\n",
      "[8,  3000] loss: 0.003\n",
      "[8,  3200] loss: 0.008\n",
      "[8,  3400] loss: 0.002\n",
      "[8,  3600] loss: 0.062\n",
      "[8,  3800] loss: 0.017\n",
      "[8,  4000] loss: 0.016\n",
      "[8,  4200] loss: 0.030\n",
      "[8,  4400] loss: 0.030\n",
      "[8,  4600] loss: 0.010\n",
      "[8,  4800] loss: 0.008\n",
      "[8,  5000] loss: 0.011\n",
      "[8,  5200] loss: 0.004\n",
      "[8,  5400] loss: 0.006\n",
      "[8,  5600] loss: 0.005\n",
      "[8,  5800] loss: 0.004\n",
      "[8,  6000] loss: 0.018\n",
      "[8,  6200] loss: 0.015\n",
      "[8,  6400] loss: 0.018\n",
      "[8,  6600] loss: 0.016\n",
      "[8,  6800] loss: 0.010\n",
      "[8,  7000] loss: 0.006\n",
      "[8,  7200] loss: 0.003\n",
      "[8,  7400] loss: 0.004\n",
      "[8,  7600] loss: 0.009\n",
      "[8,  7800] loss: 0.016\n",
      "[8,  8000] loss: 0.006\n",
      "[8,  8200] loss: 0.008\n",
      "Finished Training\n",
      "Accuracy of the network: 96 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335c6a9505b040628f9238850f6b96db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.015 MB of 0.015 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train acc</td><td>▁▅▆▇▇███</td></tr><tr><td>train loss</td><td>█▅▃▂▂▁▁▁</td></tr><tr><td>validation acc</td><td>▁▇▇█▇███</td></tr><tr><td>validation loss</td><td>█▂▂▁▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test acc</td><td>96</td></tr><tr><td>train acc</td><td>0.93136</td></tr><tr><td>train loss</td><td>0.12048</td></tr><tr><td>validation acc</td><td>0.98503</td></tr><tr><td>validation loss</td><td>0.03972</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">proud-star-21</strong> at: <a href='https://wandb.ai/Intro-ML/Voice-Recognition/runs/63tqzqzp' target=\"_blank\">https://wandb.ai/Intro-ML/Voice-Recognition/runs/63tqzqzp</a><br/> View project at: <a href='https://wandb.ai/Intro-ML/Voice-Recognition' target=\"_blank\">https://wandb.ai/Intro-ML/Voice-Recognition</a><br/>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250127_214228-63tqzqzp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ignacy-waluzenicz/code/iml/ML-intro/wandb/run-20250127_223009-c0noht9b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/Intro-ML/Voice-Recognition/runs/c0noht9b' target=\"_blank\">amber-morning-22</a></strong> to <a href='https://wandb.ai/Intro-ML/Voice-Recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/Intro-ML/Voice-Recognition' target=\"_blank\">https://wandb.ai/Intro-ML/Voice-Recognition</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/Intro-ML/Voice-Recognition/runs/c0noht9b' target=\"_blank\">https://wandb.ai/Intro-ML/Voice-Recognition/runs/c0noht9b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   200] loss: 0.019\n",
      "[1,   400] loss: 0.007\n",
      "[1,   600] loss: 0.016\n",
      "[1,   800] loss: 0.001\n",
      "[1,  1000] loss: 0.005\n",
      "[1,  1200] loss: 0.007\n",
      "[1,  1400] loss: 0.005\n",
      "[1,  1600] loss: 0.006\n",
      "[1,  1800] loss: 0.007\n",
      "[1,  2000] loss: 0.004\n",
      "[1,  2200] loss: 0.003\n",
      "[1,  2400] loss: 0.002\n",
      "[1,  2600] loss: 0.003\n",
      "[1,  2800] loss: 0.003\n",
      "[1,  3000] loss: 0.003\n",
      "[1,  3200] loss: 0.001\n",
      "[1,  3400] loss: 0.002\n",
      "[1,  3600] loss: 0.007\n",
      "[1,  3800] loss: 0.002\n",
      "[1,  4000] loss: 0.002\n",
      "[1,  4200] loss: 0.005\n",
      "[1,  4400] loss: 0.004\n",
      "[1,  4600] loss: 0.005\n",
      "[1,  4800] loss: 0.008\n",
      "[1,  5000] loss: 0.001\n",
      "[1,  5200] loss: 0.003\n",
      "[1,  5400] loss: 0.005\n",
      "[1,  5600] loss: 0.005\n",
      "[1,  5800] loss: 0.006\n",
      "[1,  6000] loss: 0.003\n",
      "[1,  6200] loss: 0.004\n",
      "[1,  6400] loss: 0.006\n",
      "[1,  6600] loss: 0.008\n",
      "[1,  6800] loss: 0.003\n",
      "[1,  7000] loss: 0.003\n",
      "[1,  7200] loss: 0.009\n",
      "[1,  7400] loss: 0.003\n",
      "[1,  7600] loss: 0.005\n",
      "[1,  7800] loss: 0.009\n",
      "[1,  8000] loss: 0.003\n",
      "[1,  8200] loss: 0.005\n",
      "[2,   200] loss: 0.006\n",
      "[2,   400] loss: 0.002\n",
      "[2,   600] loss: 0.002\n",
      "[2,   800] loss: 0.003\n",
      "[2,  1000] loss: 0.001\n",
      "[2,  1200] loss: 0.003\n",
      "[2,  1400] loss: 0.004\n",
      "[2,  1600] loss: 0.003\n",
      "[2,  1800] loss: 0.003\n",
      "[2,  2000] loss: 0.002\n",
      "[2,  2200] loss: 0.004\n",
      "[2,  2400] loss: 0.003\n",
      "[2,  2600] loss: 0.004\n",
      "[2,  2800] loss: 0.004\n",
      "[2,  3000] loss: 0.002\n",
      "[2,  3200] loss: 0.003\n",
      "[2,  3400] loss: 0.001\n",
      "[2,  3600] loss: 0.003\n",
      "[2,  3800] loss: 0.002\n",
      "[2,  4000] loss: 0.002\n",
      "[2,  4200] loss: 0.002\n",
      "[2,  4400] loss: 0.003\n",
      "[2,  4600] loss: 0.002\n",
      "[2,  4800] loss: 0.002\n",
      "[2,  5000] loss: 0.002\n",
      "[2,  5200] loss: 0.005\n",
      "[2,  5400] loss: 0.004\n",
      "[2,  5600] loss: 0.003\n",
      "[2,  5800] loss: 0.003\n",
      "[2,  6000] loss: 0.005\n",
      "[2,  6200] loss: 0.002\n",
      "[2,  6400] loss: 0.000\n",
      "[2,  6600] loss: 0.001\n",
      "[2,  6800] loss: 0.003\n",
      "[2,  7000] loss: 0.003\n",
      "[2,  7200] loss: 0.003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     24\u001b[0m training_loss\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 25\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/iml/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    583\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/iml/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m _engine_run_backward(\n\u001b[1;32m    348\u001b[0m     tensors,\n\u001b[1;32m    349\u001b[0m     grad_tensors_,\n\u001b[1;32m    350\u001b[0m     retain_graph,\n\u001b[1;32m    351\u001b[0m     create_graph,\n\u001b[1;32m    352\u001b[0m     inputs,\n\u001b[1;32m    353\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    354\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    355\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/iml/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "import torch.optim as optim\n",
    "for params_set in params:\n",
    "    net = Net()\n",
    "\n",
    "    net.to(device)\n",
    "    wandb.init(project=\"Voice-Recognition\", config={\n",
    "        \"learning_rate\": params_set[\"lr\"],\n",
    "        \"momentum\": params_set[\"momentum\"],\n",
    "        \"epochs\": epochs,\n",
    "        \"architecture\": \"CNN\"\n",
    "    }, group=\"model comps\", reinit=True)\n",
    "    wandb.run.name = params_set[\"title\"]\n",
    "    wandb.run.save()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=params_set[\"lr\"], momentum=params_set[\"momentum\"])\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    training_loss = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            training_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if i % 200 == 199:    # print every 200 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 200:.3f}')\n",
    "                running_loss = 0.0\n",
    "        wandb.log({f\"train loss\": np.mean(training_loss), \"train acc\": correct_train / total_train})\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        loss = []\n",
    "        # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        with torch.no_grad():\n",
    "            for data in validloader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = net(images)\n",
    "                current_loss = criterion(outputs, labels)\n",
    "                loss.append(current_loss.item())\n",
    "                # the class with the highest energy is what we choose as prediction\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "            wandb.log({\"validation loss\": np.mean(loss), \"validation acc\": correct / total})\n",
    "    print('Finished Training')\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = net(images)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    print(f'Accuracy of the network: {100 * correct // total} %')\n",
    "    wandb.run.summary[\"test acc\"] = 100 * correct // total\n",
    "    wandb.log({\"confusion matrix\": wandb.plot.confusion_matrix(probs=None,\n",
    "                                                                y_true=all_labels,\n",
    "                                                                preds=all_predictions,\n",
    "                                                                class_names=[\"class 0\", \"class 1\"])})\n",
    "    wandb.run.finish()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be000326c53d44bab4621a64f6855830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.004 MB of 0.004 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m wandb\u001b[38;5;241m.\u001b[39mrun\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/anaconda3/envs/iml/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:451\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mtermwarn(message, repeat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    449\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mDummy()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/iml/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:393\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/iml/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:2149\u001b[0m, in \u001b[0;36mRun.finish\u001b[0;34m(self, exit_code, quiet)\u001b[0m\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m quiet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2142\u001b[0m     deprecate\u001b[38;5;241m.\u001b[39mdeprecate(\n\u001b[1;32m   2143\u001b[0m         field_name\u001b[38;5;241m=\u001b[39mdeprecate\u001b[38;5;241m.\u001b[39mDeprecated\u001b[38;5;241m.\u001b[39mrun__finish_quiet,\n\u001b[1;32m   2144\u001b[0m         warning_message\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         ),\n\u001b[1;32m   2148\u001b[0m     )\n\u001b[0;32m-> 2149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finish(exit_code)\n",
      "File \u001b[0;32m~/anaconda3/envs/iml/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:2179\u001b[0m, in \u001b[0;36mRun._finish\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_finished \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_atexit_cleanup(exit_code\u001b[38;5;241m=\u001b[39mexit_code)\n\u001b[1;32m   2181\u001b[0m     \u001b[38;5;66;03m# Run hooks that should happen after the last messages to the\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m     \u001b[38;5;66;03m# internal service, like detaching the logger.\u001b[39;00m\n\u001b[1;32m   2183\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_teardown_hooks:\n",
      "File \u001b[0;32m~/anaconda3/envs/iml/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:2432\u001b[0m, in \u001b[0;36mRun._atexit_cleanup\u001b[0;34m(self, exit_code)\u001b[0m\n\u001b[1;32m   2429\u001b[0m         os\u001b[38;5;241m.\u001b[39mremove(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings\u001b[38;5;241m.\u001b[39mresume_fname)\n\u001b[1;32m   2431\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 2432\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_finish()\n\u001b[1;32m   2434\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   2435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mwandb_agent\u001b[38;5;241m.\u001b[39m_is_running():  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/iml/lib/python3.12/site-packages/wandb/sdk/wandb_run.py:2681\u001b[0m, in \u001b[0;36mRun._on_finish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2674\u001b[0m exit_handle\u001b[38;5;241m.\u001b[39madd_probe(on_probe\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_probe_exit)\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m progress\u001b[38;5;241m.\u001b[39mprogress_printer(\n\u001b[1;32m   2677\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_printer,\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_settings,\n\u001b[1;32m   2679\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m progress_printer:\n\u001b[1;32m   2680\u001b[0m     \u001b[38;5;66;03m# Wait for the run to complete.\u001b[39;00m\n\u001b[0;32m-> 2681\u001b[0m     _ \u001b[38;5;241m=\u001b[39m exit_handle\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m   2682\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   2683\u001b[0m         on_progress\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m   2684\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_progress_exit,\n\u001b[1;32m   2685\u001b[0m             progress_printer,\n\u001b[1;32m   2686\u001b[0m         ),\n\u001b[1;32m   2687\u001b[0m     )\n\u001b[1;32m   2689\u001b[0m \u001b[38;5;66;03m# Print some final statistics.\u001b[39;00m\n\u001b[1;32m   2690\u001b[0m poll_exit_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_poll_exit()\n",
      "File \u001b[0;32m~/anaconda3/envs/iml/lib/python3.12/site-packages/wandb/sdk/lib/mailbox.py:279\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface\u001b[38;5;241m.\u001b[39m_transport_keepalive_failed():\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MailboxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 279\u001b[0m found, abandoned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slot\u001b[38;5;241m.\u001b[39m_get_and_clear(timeout\u001b[38;5;241m=\u001b[39mwait_timeout)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# Always update progress to 100% when done\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m on_progress \u001b[38;5;129;01mand\u001b[39;00m progress_handle \u001b[38;5;129;01mand\u001b[39;00m progress_sent:\n",
      "File \u001b[0;32m~/anaconda3/envs/iml/lib/python3.12/site-packages/wandb/sdk/lib/mailbox.py:126\u001b[0m, in \u001b[0;36m_MailboxSlot._get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_and_clear\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Optional[pb\u001b[38;5;241m.\u001b[39mResult], \u001b[38;5;28mbool\u001b[39m]:\n\u001b[1;32m    125\u001b[0m     found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait(timeout\u001b[38;5;241m=\u001b[39mtimeout):\n\u001b[1;32m    127\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    128\u001b[0m             found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m~/anaconda3/envs/iml/lib/python3.12/site-packages/wandb/sdk/lib/mailbox.py:122\u001b[0m, in \u001b[0;36m_MailboxSlot._wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout: \u001b[38;5;28mfloat\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event\u001b[38;5;241m.\u001b[39mwait(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[0;32m~/anaconda3/envs/iml/lib/python3.12/threading.py:655\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    653\u001b[0m signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flag\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 655\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cond\u001b[38;5;241m.\u001b[39mwait(timeout)\n\u001b[1;32m    656\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/anaconda3/envs/iml/lib/python3.12/threading.py:359\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 359\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mTrue\u001b[39;00m, timeout)\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wandb.run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
